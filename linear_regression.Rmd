---
title: "P8130 Final Report"
date: "2020/12/8"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(corrplot)
library(HH)
library(leaps)
library(olsrr)
library(mgcv)
library(modelr)
library(arsenal)

set.seed(1)

knitr::opts_chunk$set(
 echo = TRUE,
 warning = F,   
 message = F,
 fig.width = 6,
 fig.height = 4,
 # fig.asp = 0.618,
 out.width = "100%")
```

## Import dataset

```{r}
hate_crime_data = 
  read_csv("./HateCrimes.csv") %>% 
  janitor::clean_names()
```

## Inspect NAs

```{r}
hate_crime_data %>% 
  sapply(.,function(x) sum(is.na(x)))
```

Drop NAs

```{r}
hate_crime_df = 
  hate_crime_data %>% 
  naniar::replace_with_na(
    replace = list(hate_crimes_per_100k_splc = "N/A")
    ) %>% 
  drop_na() %>% 
  mutate(
    hate_crimes_per_100k_splc = as.numeric(hate_crimes_per_100k_splc),
    unemployment = factor(unemployment, levels = c("low","high")),
    urbanization = factor(urbanization, levels = c("low","high"))
  ) 
```

## Descriptive Data Analysis

### Catrgorical Variables

Unemployment

```{r}
hate_crime_df %>% 
  group_by(unemployment) %>% 
  summarise(
    count = n(),
    percentage = n()/nrow(hate_crime_df)
  ) %>% 
  knitr::kable(digits = 2)
```

Urbanization

```{r}
hate_crime_df %>% 
  group_by(urbanization) %>% 
  summarise(
    count = n(),
    percentage = n()/nrow(hate_crime_df)
  ) %>% 
  knitr::kable(digits = 2)
```



```{r}
gini_level = 
  hate_crime_df %>% 
  mutate(
    gini_index = case_when(
     gini_index >= 0.455 ~ 'High',
     gini_index < 0.455 ~ 'Low'
    ))


my_controls = tableby.control(
  test = T,
  numeric.stats = c("meansd", "medianq1q3", "range"),
  cat.stats = c("countpct", "Nmiss2"),
  stats.labels = list(
    meansd = "Mean (SD)",
    medianq1q3 = "Median (Q1, Q3)",
    range = "Min - Max"
  )
)

table_one <- tableby(
  gini_index ~ urbanization + unemployment + median_household_income + perc_population_with_high_school_degree + perc_non_citizen + perc_non_white + hate_crimes_per_100k_splc, 
  data = gini_level,
  control = my_controls
)

summary(table_one,
        title = "Summary Statistic by level of hate crime rate", text = T) 
```
We can find there is potential for confounding or interactivity in unemployment, perc_population_with_high_school_degree,perc_non_white as level of hate crime rate have large difference in distribution in these variable.


Below is a summary table of all continuous variables group by unemployment status.

```{r, results="asis"}
my_controls = tableby.control(
  test = T,
  numeric.stats = c("meansd", "medianq1q3", "range"),
  cat.stats = c("countpct", "Nmiss2"),
  stats.labels = list(
    meansd = "Mean (SD)",
    medianq1q3 = "Median (Q1, Q3)",
    range = "Min - Max"
  )
)

table_one <- tableby(
  unemployment ~ hate_crimes_per_100k_splc + median_household_income + perc_population_with_high_school_degree + perc_non_citizen + perc_non_white + gini_index, 
  data = hate_crime_df,
  control = my_controls
)

summary(table_one,
        title = "Summary Statistic by Unemployment", text = T) 
```

Then a summary table of all continuous variables group by urbanization status.

```{r, results="asis"}
table_two <- tableby(
  urbanization ~ hate_crimes_per_100k_splc + median_household_income + perc_population_with_high_school_degree + perc_non_citizen + perc_non_white + gini_index, 
  data = hate_crime_df,
  control = my_controls
)

summary(table_two,
        title = "Summary Statistic by Urbanization", text = T) 
```

## Normality Test

Histogram

```{r}
hate_crime_df %>% 
  ggplot(aes(x = hate_crimes_per_100k_splc, y = ..density..)) +
  geom_histogram(colour = "black", alpha = 0.1) +
  geom_density(alpha = 0.4) +
  labs(
    x = "hate crime rate",
    title = "Distribution of Hate Crime Rate"
  )
```

qq plot

```{r}
qqnorm(hate_crime_df$hate_crimes_per_100k_splc, col=2, pch=19, cex=1.5)
qqline(hate_crime_df$hate_crimes_per_100k_splc, col = 1,lwd=2,lty=2)
```

The distribution of hate crime rate is heavily right skewed.

## Tansformation

distribution of ln(hate_crimes_per_100k_splc)

```{r}
trans_hate_crime_df =
hate_crime_df %>% 
  mutate(
    hate_crimes_per_100k_splc = 
      log(hate_crimes_per_100k_splc)
  ) 

trans_hate_crime_df %>% 
  ggplot(aes(x = hate_crimes_per_100k_splc, y = ..density..)) +
  geom_histogram(binwidth = 0.1, colour = "black", alpha = 0.1) +
  geom_density(alpha = 0.4) +
  labs(
    x = "log hate crime rate",
    title = "Distribution of Log Hate Crime Rate"
  )
```

qq plot

```{r}
qqnorm(trans_hate_crime_df$hate_crimes_per_100k_splc, col=2, pch=19, cex=1.5)
qqline(trans_hate_crime_df$hate_crimes_per_100k_splc, col = 1,lwd=2,lty=2)
title(sub = "* After taking log transformation", font.sub = 3)
```

It's much better now.
 
## Outliers

Make plot for gini_index vs hate crime rate

```{r}
trans_hate_crime_df %>% 
 ggplot(aes(x = gini_index, y = hate_crimes_per_100k_splc)) +
  geom_point() +
  geom_smooth(method = "lm", se = F, color = "red") +
  labs(
    y = "ln(hate crime rate)",
    x = "gini index",
    title = "Gini Index vs Hate Crime Rate",
    caption = "Include all states except ones with NAs"
  )
```

Based on the plot above, there is positive association between gini index and hate crime rate.

Find potential influential outliers

```{r}
fit_all = lm(hate_crimes_per_100k_splc ~.-state, data = trans_hate_crime_df)

summary(fit_all)


par(mfrow=c(2,2))
plot(fit_all)
```

Notice row 9 is a potential influential point. It's the reocord for
District of Columbia, which has very high hate crime rate.

Remove observation for District of Columbia and fit the full model again.

```{r}
trans_hate_crime_df_no_dc = trans_hate_crime_df[-c(9),]

fit_all_no_dc = lm(hate_crimes_per_100k_splc ~.-state, data = trans_hate_crime_df_no_dc) 

summary(fit_all_no_dc)

par(mfrow=c(2,2))
plot(fit_all_no_dc)
```

Then take a look again at the relationship between gini index and hate crime rate

```{r}
trans_hate_crime_df_no_dc %>% 
 ggplot(aes(x = gini_index, y = hate_crimes_per_100k_splc)) +
  geom_point() +
  geom_smooth(method = "lm", se = F, color = "red") +
  labs(
    y = "ln hate crime rate",
    x = "gini index",
    title = "Gini Index vs Hate Crime Rate",
    caption = "Exclude District of Columbia"
  )
```

There is slightly negative association between gini index and hate crime rate. The regression line is nearly horizontal, implying there maybe no linear relationship between gini index and hate frime rate. Removing row 9 has changed the regression results, which means that it's influential. We should exclude row 9.
 
## Test for Collinearity 

Correlation matrix for all variables

```{r}
  trans_hate_crime_df_no_dc %>% 
  rename(
    perc_hs_degree = perc_population_with_high_school_degree,
    hate_crime_rate = hate_crimes_per_100k_splc
  ) %>% 
  dplyr::select(-state) %>% 
  mutate(
    unemployment = as.numeric(unemployment),
    urbanization = as.numeric(urbanization)
  ) %>% 
  cor(.,method = "pearson") %>% 
  round(.,digits = 2) %>% 
  head() %>% 
  knitr::kable()
```

We can see that

* `perc_non_white` and `perc_non_citizen` are highly correlated (0.73).

* `urbanization` and `perc_non_citizen` are moderately correlated (0.67)

* `perc_population_with_high_school_degree` and `median_household_income`
are moderately correlated (0.66)

* `gini_index` and `perc_population_with_high_school_degree` has moderate correlation (-0.66)

VIF

```{r}
vif(fit_all_no_dc)
```

`perc_population_with_high_school_degree` and `perc_non_citizen` has quite high vif, while none of them exceed 5. Then we test whether removing `perc_population_with_high_school_degree` influences the regression results.

```{r}
fit_all_no_degree = lm(hate_crimes_per_100k_splc ~.-state -perc_population_with_high_school_degree, data = trans_hate_crime_df_no_dc)

summary(fit_all_no_degree)
```

The estimate for median_household_income change from negative to positive. While the standard error doesn't inflated and the adjusted R_squared reduces from 0.057 to 0.027, we believe adding  `perc_population_with_high_school_degree` is not redundant.

ANOVA

```{r}
anova(fit_all_no_degree, fit_all_no_dc)
```

P_value = 0.1482>0.05

So we decide to remove `perc_non_citizen` and see the effect on regression model

```{r}
fit_all_no_citizen = lm(hate_crimes_per_100k_splc ~.-state -perc_non_citizen, data = trans_hate_crime_df_no_dc)

summary(fit_all_no_citizen)
```

After removing `perc_non_citizen`, adjusted R_squared increases from 0.057 to 0.080, meaning adding `perc_non_citizen` is redundant. 

ANOVA

```{r}
anova(fit_all_no_citizen, fit_all_no_dc)
```

P_value = 0.7718>0.05

Remove `perc_non_citizen`

```{r}
hate_crime_reg_df = 
  trans_hate_crime_df_no_dc %>% 
  dplyr::select(-perc_non_citizen)
```


## Predictor Selection

Get started with the approximately optimal number of predictors for a linear model (select by C_p, Adj R^2, BIC, AIC).

```{r}
reg_subsets = regsubsets(hate_crimes_per_100k_splc ~ .-state, data = hate_crime_reg_df)
rs = summary(reg_subsets)
rs
```

Plots of C_p, Adj-R^2 and BIC for all the best subsets

```{r}
par(mar=c(4,4,1,1))
par(mfrow=c(2,2))

plot(2:7, rs$cp, xlab="NO. of parameters", ylab="Cp Statistic")
abline(0,1)

plot(2:7, rs$adjr2, xlab="NO. of parameters", ylab="Adj R2")

plot(2:7, rs$bic, xlab="NO. of parameters", ylab="BIC")
abline(0,1)
```

Compare models with 2, 3, 4 predictors

```{r}
hate_crime_reg_df %>% 
  nest(state:hate_crimes_per_100k_splc) %>% 
  mutate(
    multi_fit_2 = 
      map(.x = data, ~lm(hate_crimes_per_100k_splc ~ perc_population_with_high_school_degree + gini_index, data=.x)),
multi_fit_3 = 
  map(.x = data, ~lm(hate_crimes_per_100k_splc ~ unemployment + perc_population_with_high_school_degree + gini_index, data=.x)),
multi_fit_4 = 
  map(.x = data, ~lm(hate_crimes_per_100k_splc ~ unemployment + perc_population_with_high_school_degree + gini_index + urbanization, data=.x))
  ) %>% 
  pivot_longer(
    multi_fit_2:multi_fit_4,
    names_to = "model",
    values_to = "results"
  ) %>% 
  mutate(
    AIC = map(.x = results, ~AIC(.x)),
    BIC = map(.x = results, ~BIC(.x)),
    C_p = map(.x = results, ~ols_mallows_cp(.x, fit_all_no_citizen)),
    adj_r_sq = map(.x = results, ~summary(.x)$adj.r.squared)
  ) %>% 
  unnest(AIC:adj_r_sq)
```

(Note: BIC is more conservative than AIC by penalizing more on the number of parameters)

Note that the adjusted R2, BIC and Cp are calculated on the training data that have been used to fit the model. This means that, the model selection, using these metrics, is possibly subject to overfitting and may not perform as well when applied to new data.

A more rigorous approach is to select a models based on the prediction error computed on a new test data using k-fold cross-validation techniques.

## K-fold CV

```{r}
cv_df = 
  hate_crime_reg_df %>% 
  crossv_mc(1000) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  ) %>% 
  mutate(
    multi_fit_2 = map(.x = train, ~lm(hate_crimes_per_100k_splc ~ perc_population_with_high_school_degree + gini_index, data=.x)),
multi_fit_3 = 
  map(.x = train, ~lm(hate_crimes_per_100k_splc ~ unemployment + perc_population_with_high_school_degree + gini_index, data=.x)),
multi_fit_4 = 
  map(.x = train, ~lm(hate_crimes_per_100k_splc ~ unemployment + perc_population_with_high_school_degree + gini_index + urbanization, data=.x))
      ) %>% 
  mutate(
    rmse_mod_2_preds = map2_dbl(multi_fit_2, test, ~rmse(model = .x, data = .y)),
    rmse_mode_3_preds = map2_dbl(multi_fit_3, test, ~rmse(model = .x, data = .y)),
    rmse_model_4_preds = map2_dbl(multi_fit_4, test, ~rmse(model = .x, data = .y)))
```

Make plots of RMSE vs model

```{r}
cv_df %>% 
   dplyr::select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

We decide to choose the model with two predictors: perc_population_with_high_school_degree and gini_index

```{r}
multi_fit_start = lm(hate_crimes_per_100k_splc ~ perc_population_with_high_school_degree + gini_index, data=hate_crime_reg_df)

broom::tidy(multi_fit_start) %>% 
  knitr::kable(digits = 3)
```

## Model Modification

### Interaction between Gini Index and Unemployment

Our previous selected model

```{r}
model_select = lm(hate_crimes_per_100k_splc ~ perc_population_with_high_school_degree + gini_index, data=hate_crime_reg_df)

summary(model_select)
```


stratify by unemployment

```{r}
hate_crime_reg_df %>% 
  ggplot(aes(x = gini_index, y = hate_crimes_per_100k_splc, color = unemployment)) +
  geom_point() +
  geom_smooth(method = "lm", se = F)
```

The two line are not parallel

Add interaction between gini index and unemployment to the model

```{r}
multi_fit = lm(hate_crimes_per_100k_splc ~  perc_population_with_high_school_degree + gini_index*unemployment, data = hate_crime_reg_df)

summary(multi_fit)
broom::tidy(multi_fit) %>% 
  knitr::kable(digits = 3)
```

ANOVA

```{r}
anova(multi_fit)
```

ANOVA Comparison

```{r}
anova(model_select,multi_fit)
```

Stratified models

Low unemployment: A positive, statistically significant association b/w gini index and hate crime rate

```{r}
reg_low_unemp_df =  
hate_crime_reg_df %>% 
  filter(unemployment == "low") 

multi_fit_low_unemp = 
  lm(hate_crimes_per_100k_splc ~ perc_population_with_high_school_degree + gini_index, data = reg_low_unemp_df)

summary(multi_fit_low_unemp)
```

High unemployment: A negative, but not statistically significant association b/w gini index and hate crime rate

```{r}
reg_high_unemp_df =  
hate_crime_reg_df %>% 
  filter(unemployment == "high") 

multi_fit_high_unemp = 
  lm(hate_crimes_per_100k_splc ~  perc_population_with_high_school_degree + gini_index, data = reg_high_unemp_df)

summary(multi_fit_high_unemp)
```

### Interaction between gini index and urbanization

stratify by urbanization

```{r}
hate_crime_reg_df %>% 
  ggplot(aes(x = gini_index, y = hate_crimes_per_100k_splc, color = urbanization)) +
  geom_point() +
  geom_smooth(method = "lm", se = F)
```

The two line are parallel. There is no interaction between gini index and urbanization

 
## K-fold CV

```{r}
cv_df_2 = 
  hate_crime_reg_df %>% 
  crossv_mc(1000) %>% 
  mutate(
    model_small = map(train, ~lm(hate_crimes_per_100k_splc ~  perc_population_with_high_school_degree + gini_index, data = .x)),
    model_large = map(train, ~lm(hate_crimes_per_100k_splc ~  perc_population_with_high_school_degree + gini_index*unemployment,data = .x))
  ) %>% 

  mutate(
    rmse_model_small = map2_dbl(model_small, test, ~rmse(model = .x, data = .y)),
    rmse_model_large = map2_dbl(model_large, test, ~rmse(model = .x, data = .y))
  )
   
``` 

Make plots of RMSE vs model

```{r}
cv_df_2 %>% 
   dplyr::select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin() 
```
 
##Model diagnostic

Check model assumption 
 
```{r}
par(mfrow=c(2,2))
plot(multi_fit)
```

Check goodness of fit

```{r}
summary(multi_fit)
```


Add residual of Large Model

```{r}
# hate_crime_reg_df %>% 
#   add_predictions(model = multi_fit, var = "pred") %>% 
#   add_residuals(model = multi_fit, var = "resid") %>%
#   ggplot(aes(x = pred, y = resid)) + 
#   geom_point(alpha = 0.5, color = "blue", size = 3) +
#   geom_smooth(se = F, color = "red", method = "lm") + 
#   labs(title = "Model residuals vs. Fitted values", 
#        y = "Residuals",
#        x = "Fitted Value")
```